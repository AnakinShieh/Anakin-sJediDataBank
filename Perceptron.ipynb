{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知机上机报告\n",
    "谢昊君 1200015556 元培学院\n",
    "感知机算法解决二元分类问题，分类见training data.\n",
    "\n",
    "## 感知机算法简介\n",
    "感知机(Perceptron)为一种二类分类的线性分类模型，输入为实例的特征向量，输出为实例类别，一般取+1、-1。对应于输入空间将实例进行线性划分的分离超平面。\n",
    "\n",
    "假设输入空间$\\mathscr{X}\\subsetneqq\\mathbf{R}^n$,输出空间$\\mathscr{Y}=\\{+1,-1\\}$\n",
    "$$f(x)=sign(wx+b)$$\n",
    "\n",
    "\n",
    "$sign(x)$此处为符号函数，表示如下：\n",
    "$$sign(x)=+1, x\\geq 0;-1,x\\lt0$$\n",
    "\n",
    "### 数据线性可分性的定义:\n",
    "\n",
    "给定数据集$T = \\{(x_1,y_1),(x_2，y_2),....\\}$, 其中$x_1\\in\\mathscr{X}=\\mathbf{R}^n$,$\\mathscr{Y}=\\{+1,-1\\}$，若存在某个超平面S:$wx+b=0$使得数据集中所有正实例点和负实例点正确划分到超平面两侧。\n",
    "对$y_i=+1$,有$wx_i+b\\gt0$;对$y_i=-1$,有$wx_i+b<0$，此时称数据集为线性可分。\n",
    "\n",
    "### 损失函数-\n",
    "$$L(w,b)=-\\sum_{x_i\\in M}y_i(wx_i+b)$$\n",
    "其中M指误分类点集合。对正确分类的点，损失函数为0\n",
    "对损失函数进行随机梯度下降法进行优化，即得更新标准。\n",
    "\n",
    "对$w$有：\n",
    "$$w\\leftarrow w+\\eta y_i x_i$$\n",
    "对$b$有：\n",
    "$$b\\leftarrow b+\\eta y_i$$\n",
    "\n",
    "其中$\\eta$为学习率，即最优化中的步长。\n",
    "\n",
    "### 对偶形式的感知机\n",
    "略\n",
    "## 本文采取方法具体说明\n",
    "（本文采用方法假定了训练集为线性可分集合）\n",
    "在初始的权重向量设定下，计算此时的分类。函数为 $f(x)=wx-\\theta$ （$\\theta$与上文符号相反）\n",
    "\n",
    "选取分类错误的样本中的第一个作为标准，更新权重向量。直到所有样本都正确分类。\n",
    "\n",
    "按照老师讲义上的更新标准 $w_i(k+1)=w_i(k)+\\eta(t(k)-y(k))x_i(k)$,在二元分类的简单感知机模型下，相当于上一部分简介中以$w\\leftarrow w+2*\\eta y_i x_i$标准更新权重矩阵，对应不同的学习率。本文采取方法依照老师讲义上为准。\n",
    "\n",
    "\n",
    "## 实验结果\n",
    "以8个样本点，采用0作为初始和权重向量后，以每次误分类的第一个样本为标准更新，最终将所有样本正确分类。权重矩阵结果如下。\n",
    "\n",
    "| $x_1$|$x_2$ |$x_3$ |$\\theta$|\n",
    "|------|------|------|------|\n",
    "| -0.4 | -0.2| -0.6 | -0.6|\n",
    "\n",
    "即最终的分类函数为:\n",
    "$$f(x)=-0.4x_1-0.2x_2-0.6x_3+0.6$$-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from random import choice\n",
    "training_data = [[0,0,0,1],[1,0,0,1],[0,1,0,1],[0,0,1,1],[1,1,0,1],[1,0,1,-1],[0,1,1,-1],[1,1,1,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  1]\n",
      " [ 1  0  0  1]\n",
      " [ 0  1  0  1]\n",
      " [ 0  0  1  1]\n",
      " [ 1  1  0  1]\n",
      " [ 1  0  1 -1]\n",
      " [ 0  1  1 -1]\n",
      " [ 1  1  1 -1]]\n"
     ]
    }
   ],
   "source": [
    "training_data = np.array(training_data)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 前三列为特征，最后未一列为分类结果，-1或+1\n",
    "feature = training_data[:,0:3]\n",
    "output = training_data[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义signal函数，将大于等于0定义为1，小于0定义为-1\n",
    "def sign(x):\n",
    "    return (x>=0)*2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set initial weight and theta\n",
    "# 权重向量选取可以随机一个比较小的值。此处直接选择为0.也可以写作：\n",
    "# w = rd.rand(1,3)\n",
    "# theta = rd.rand(1,1)\n",
    "w = np.array([0,0,0])\n",
    "theta = 0\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 感知机算法中误差项的定义，因为只有两种分类，+1，-1，若分类正确则误差为0，若分类错误误差为+-2\n",
    "errors = sign(feature.dot(w)-theta)- output\n",
    "zero = np.zeros((feature.shape[0]))\n",
    "# 算法训练直到所有误差项均为0为止((errors == 0).all() )，即上式为False时继续循环。\n",
    "while (errors == 0).all() ==False:\n",
    "    # 选取未正确分类的样本\n",
    "    tmp = (errors == zero)\n",
    "    # 每次均选取第一个未正确分类对象调整\n",
    "    a = (feature[tmp!=True])[0]\n",
    "    expected = output[tmp!=True][0]\n",
    "    # 按照感知机算法的更新标准进行，\n",
    "    w = w + learning_rate * (expected)*2*a\n",
    "    theta = theta - learning_rate * expected * 2\n",
    "    errors = sign(feature.dot(w)-theta)- output\n",
    "    \n",
    "# 此处算法亦可简化为感知机算法的对偶算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4 -0.2 -0.6]\n",
      "-0.6\n",
      "[0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#输出训练完毕后的权重向量、常数项和最后的误差项。\n",
    "#结果显示所有\n",
    "print (w)\n",
    "print (theta)\n",
    "print (errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# as classes\n",
    "# 将以上写法改写为类的形式。\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.1\n",
    "#         self.training_feature = training_feature\n",
    "#         self.training_class = training_class\n",
    "        self.w = []\n",
    "        self.theta = []\n",
    "        \n",
    "    # 允许自定义learning rate\n",
    "    def set_learning_rate(self,x=0.1):\n",
    "        self.learning_rate = x\n",
    "        \n",
    "    def train(self,feature,output):\n",
    "        wide = feature.shape[1]\n",
    "        w = np.zeros((wide))\n",
    "        theta = 0\n",
    "        errors = sign(feature.dot(w)-theta)- output\n",
    "        zero = np.zeros((feature.shape[0]))\n",
    "        while (errors == 0).all() ==False:\n",
    "            tmp = (errors == zero)\n",
    "            a = (feature[tmp!=True])[0]\n",
    "            expected = output[tmp!=True][0]\n",
    "            w = w + self.learning_rate * (expected)*2*a\n",
    "            theta = theta - self.learning_rate * expected * 2\n",
    "            errors = sign(feature.dot(w)-theta)- output\n",
    "        self.w = w\n",
    "        self.theta = theta\n",
    "    \n",
    "    # 定义此算法下预测的过程。\n",
    "    def predict(self,feature):\n",
    "        return sign(feature.dot(self.w)-self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成Perceptron对象并做测试\n",
    "a = Perceptron()\n",
    "a.set_learning_rate(0.3)\n",
    "a.train(feature,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(a.predict(np.array([[1,2,1],[1,3,1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6 -0.6 -1.2]\n",
      "-1.2\n"
     ]
    }
   ],
   "source": [
    "print(a.w)\n",
    "print(a.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = Perceptron()\n",
    "b.train(feature,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(b.predict(feature))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
